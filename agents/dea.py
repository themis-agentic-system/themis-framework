"""Implementation of the Doctrinal Evaluation Agent (DEA)."""

from __future__ import annotations

from collections.abc import Callable, Iterable
from typing import Any

from agents.base import BaseAgent
from agents.tooling import ToolSpec
from tools.llm_client import get_llm_client


class DEAAgent(BaseAgent):
    """Produce legal theories, citations, and doctrinal analysis."""

    REQUIRED_TOOLS = ("issue_spotter", "citation_retriever")

    def __init__(
        self,
        *,
        tools: Iterable[ToolSpec] | dict[str, Callable[..., Any]] | None = None,
    ) -> None:
        super().__init__(name="dea")
        default_tools = [
            ToolSpec(
                name="issue_spotter",
                description="Identify potential legal issues from facts and objectives.",
                fn=_default_issue_spotter,
                input_schema={"type": "object"},
                output_schema={"type": "array"},
            ),
            ToolSpec(
                name="citation_retriever",
                description="Locate supporting legal authorities for spotted issues.",
                fn=_default_citation_retriever,
                input_schema={"type": "object"},
                output_schema={"type": "array"},
            ),
        ]
        self.register_tools(default_tools)

        if tools:
            if isinstance(tools, dict):
                self.register_tools(list(tools.items()))
            else:
                self.register_tools(tools)

        self.require_tools(self.REQUIRED_TOOLS)

    async def _run(self, matter: dict[str, Any]) -> dict[str, Any]:
        """Autonomously identify legal issues and research authorities.

        Claude decides which tools to use and in what order based on the matter data.
        """
        import json

        llm = get_llm_client()

        # Define available tools in Anthropic format
        tools = [
            {
                "name": "issue_spotter",
                "description": "Identify potential legal issues from facts and objectives. Use this to analyze the fact pattern and spot all legal issues.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "matter": {
                            "type": "object",
                            "description": "Full matter object containing facts, parties, and context"
                        }
                    },
                    "required": ["matter"]
                }
            },
            {
                "name": "citation_retriever",
                "description": "Locate supporting legal authorities for spotted issues. Use this after identifying issues to find relevant case law and statutes.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "matter": {
                            "type": "object",
                            "description": "Matter object with context"
                        },
                        "issues": {
                            "type": "array",
                            "description": "List of identified legal issues to research"
                        }
                    },
                    "required": ["matter", "issues"]
                }
            }
        ]

        # Map tool names to actual functions
        tool_functions = {
            "issue_spotter": lambda matter: _default_issue_spotter(matter),
            "citation_retriever": lambda matter, issues: _default_citation_retriever(matter, issues),
        }

        # Let Claude autonomously decide which tools to use
        system_prompt = """You are DEA (Doctrinal & Equitable Analysis), an expert at identifying legal issues and researching controlling authority.

Your role:
1. Analyze fact patterns to identify all potential legal issues
2. Research relevant case law, statutes, and authorities for each issue
3. Distinguish between controlling and contrary authorities
4. Synthesize a comprehensive legal analysis
5. Flag gaps requiring additional research

Use the available tools intelligently based on what data is present in the matter.
After using tools, provide your final analysis as a JSON object with these fields:
- issues: Array of legal issues identified
- authorities: Array of relevant citations and authorities
- controlling_authorities: Array of controlling authority citations
- contrary_authorities: Array of contrary authority citations
- analysis: Comprehensive legal analysis narrative

Be thorough in issue spotting and diligent in authority research."""

        user_prompt = f"""Analyze this legal matter and identify all legal issues with supporting authorities:

MATTER DATA:
{json.dumps(matter, indent=2)}

Use the available tools to:
1. Identify all potential legal issues from the facts
2. Research relevant authorities for each issue
3. Distinguish controlling from contrary authority

Then provide your complete legal analysis."""

        # Claude autonomously uses tools
        result = await llm.generate_with_tools(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            tools=tools,
            tool_functions=tool_functions,
            max_tokens=4096,
        )

        # Track tool invocations for metrics
        # Since we're using generate_with_tools which bypasses _call_tool,
        # we need to manually track tool invocations
        if "tool_calls" in result and result["tool_calls"]:
            self._tool_invocations += len(result["tool_calls"])

        # Parse Claude's final response
        try:
            response_text = result["result"]
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            if json_start != -1 and json_end > json_start:
                analysis_payload = json.loads(response_text[json_start:json_end])
            else:
                # Fallback: construct from tool calls
                analysis_payload = self._construct_analysis_from_tool_calls(result["tool_calls"], matter)
        except (json.JSONDecodeError, KeyError):
            # Fallback to constructing from tool calls
            analysis_payload = self._construct_analysis_from_tool_calls(result["tool_calls"], matter)

        # Extract components
        spotted_issues = analysis_payload.get("issues", [])
        citations = analysis_payload.get("authorities", [])
        controlling_auths = analysis_payload.get("controlling_authorities", [])
        contrary_auths = analysis_payload.get("contrary_authorities", [])
        analysis_text = analysis_payload.get("analysis", "")

        # If not provided in payload, derive from citations
        if not controlling_auths:
            controlling_auths = [c.get("cite", "") for c in citations if c.get("cite")]

        # Track unresolved issues
        unresolved: list[str] = []
        if not spotted_issues:
            unresolved.append("No legal issues identified from the fact pattern.")
        if not citations:
            unresolved.append("Unable to locate supporting authorities for the issues raised.")
        if not contrary_auths:
            unresolved.append("No contrary authority identified - consider researching opposing arguments.")

        legal_analysis = {
            "issues": spotted_issues,
            "authorities": citations,
            "analysis": analysis_text or await _synthesise_analysis(spotted_issues, citations, matter),
        }

        authorities_signal = {
            "controlling_authorities": controlling_auths,
            "contrary_authorities": contrary_auths or ["None identified - further research recommended"],
        }

        provenance = {
            "tools_used": [tc["tool"] for tc in result["tool_calls"]],
            "tool_rounds": result["rounds"],
            "autonomous_mode": True,
            "citations_considered": [
                citation.get("cite") if isinstance(citation, dict) else str(citation)
                for citation in citations
                if citation
            ],
        }

        return self._build_response(
            core={
                "legal_analysis": legal_analysis,
                "authorities": authorities_signal,
            },
            provenance=provenance,
            unresolved_issues=unresolved,
        )

    def _construct_analysis_from_tool_calls(self, tool_calls: list[dict], matter: dict[str, Any]) -> dict[str, Any]:
        """Fallback: construct analysis payload from tool call results."""
        issues = []
        authorities = []

        for tc in tool_calls:
            if tc["tool"] == "issue_spotter" and isinstance(tc["result"], list):
                issues = tc["result"]
            elif tc["tool"] == "citation_retriever" and isinstance(tc["result"], list):
                authorities = tc["result"]

        return {
            "issues": issues,
            "authorities": authorities,
            "controlling_authorities": [a.get("cite", "") for a in authorities if a.get("cite")],
            "contrary_authorities": [],
            "analysis": ""
        }


async def _default_issue_spotter(matter: dict[str, Any]) -> list[dict[str, Any]]:
    """Identify legal issues using LLM analysis of the matter."""
    llm = get_llm_client()

    # Build context from matter
    context_parts = []
    if matter.get("summary") or matter.get("description"):
        context_parts.append(f"Matter: {matter.get('summary') or matter.get('description')}")
    if matter.get("parties"):
        context_parts.append(f"Parties: {', '.join(matter.get('parties', []))}")

    # Include facts if available from LDA output
    facts = matter.get("facts", {})
    if isinstance(facts, dict) and facts.get("fact_pattern_summary"):
        fact_list = facts["fact_pattern_summary"]
        if fact_list:
            context_parts.append("Key Facts:\n" + "\n".join(f"- {fact}" for fact in fact_list[:10]))

    context = "\n\n".join(context_parts)

    system_prompt = """You are a skilled legal analyst specializing in issue spotting. Your job is to:
1. Analyze fact patterns and identify all potential legal issues
2. Categorize issues by area of law (tort, contract, property, etc.)
3. Link each issue to the specific facts that give rise to it
4. Assess the strength/merit of each issue

Be thorough but focused on legally significant issues."""

    user_prompt = f"""Analyze this legal matter and identify all potential legal issues.

{context}

For each legal issue you identify, provide:
1. The issue name/description
2. The area of law (e.g., "Tort - Negligence", "Contract - Breach")
3. Supporting facts from the matter
4. A brief assessment of the issue's strength (strong/moderate/weak)

Respond in JSON format:
{{
  "issues": [
    {{
      "issue": "issue description",
      "area_of_law": "legal area",
      "facts": ["supporting fact 1", "supporting fact 2"],
      "strength": "strong/moderate/weak"
    }}
  ]
}}"""

    response_format = {
        "issues": [
            {
                "issue": "string",
                "area_of_law": "string",
                "facts": ["string"],
                "strength": "string",
            }
        ]
    }

    try:
        result = await llm.generate_structured(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            response_format=response_format,
        )
        import logging
        logger = logging.getLogger("themis.agents.dea")
        logger.info(f"Issue spotter LLM response: {result}")
        issues = result.get("issues", [])
        logger.info(f"Extracted {len(issues)} issues from LLM response")
        return issues
    except Exception as e:
        # Log the error so we can see what's failing
        import logging
        logger = logging.getLogger("themis.agents.dea")
        logger.error(f"Issue spotter LLM call failed: {e!s}", exc_info=True)

        # Fallback to extracting from matter payload
        issues: list[dict[str, Any]] = []
        for entry in matter.get("issues", []):
            if isinstance(entry, str):
                issues.append({"issue": entry, "facts": [], "area_of_law": "Unknown"})
            elif isinstance(entry, dict):
                issue_label = entry.get("issue") or entry.get("label")
                if issue_label:
                    issues.append(
                        {
                            "issue": issue_label,
                            "facts": entry.get("facts", []),
                            "area_of_law": entry.get("area_of_law", "Unknown"),
                        }
                    )
        return issues


def _default_citation_retriever(
    matter: dict[str, Any],
    issues: list[dict[str, Any]] | list[str],
) -> list[dict[str, Any]]:
    """Return a list of candidate authorities relevant to the issues.

    Args:
        matter: Matter dictionary with context
        issues: List of issues (can be dicts with 'issue' key or plain strings)
    """

    authorities = []
    for authority in matter.get("authorities", []):
        if isinstance(authority, dict) and authority.get("cite"):
            authorities.append(authority)
        elif isinstance(authority, str):
            authorities.append({"cite": authority, "summary": "Reference provided as free text."})

    if not authorities and issues:
        for issue in issues:
            # Handle both dict and string issue formats
            if isinstance(issue, dict):
                issue_text = issue.get('issue', str(issue))
            else:
                issue_text = str(issue)

            authorities.append(
                {
                    "cite": f"Secondary research required for issue: {issue_text}",
                    "summary": "No authority supplied; follow-up research needed.",
                }
            )

    return authorities


async def _synthesise_analysis(
    issues: list[dict[str, Any]],
    citations: list[dict[str, Any]],
    matter: dict[str, Any],
) -> str:
    """Create a legal analysis narrative using LLM."""
    if not issues:
        return "No issues identified to analyse."

    llm = get_llm_client()

    # Build context
    issues_text = "\n".join(
        f"- {i+1}. {issue.get('issue')} (Area: {issue.get('area_of_law', 'N/A')}, "
        f"Strength: {issue.get('strength', 'N/A')})"
        for i, issue in enumerate(issues)
    )

    citations_text = "\n".join(
        f"- {cite.get('cite')}: {cite.get('summary', 'No summary')}"
        for cite in citations
        if cite.get("cite")
    )

    system_prompt = """You are a legal expert writing doctrinal analysis. Your job is to:
1. Synthesize legal issues and authorities into a coherent analysis
2. Apply legal precedents to the facts
3. Explain how each authority supports or undermines each issue
4. Provide a clear, professional legal analysis

Write in a formal but clear legal style."""

    user_prompt = f"""Write a legal analysis synthesizing these issues and authorities.

Matter Context: {matter.get('summary') or matter.get('description', 'N/A')}
Parties: {', '.join(matter.get('parties', ['N/A']))}

Legal Issues Identified:
{issues_text}

Authorities:
{citations_text if citations_text else 'No specific authorities cited - requires research'}

Provide a comprehensive legal analysis (3-5 paragraphs) that:
1. Analyzes each legal issue in light of the facts
2. Applies relevant authorities to support or challenge each issue
3. Identifies gaps requiring additional research
4. Provides preliminary conclusions on the strength of each claim"""

    try:
        analysis = await llm.generate_text(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_tokens=2000,
        )
        return analysis
    except Exception as e:
        # Log the error so we can see what's failing
        import logging
        logger = logging.getLogger("themis.agents.dea")
        logger.error(f"Analysis synthesis LLM call failed: {e!s}", exc_info=True)

        # Fallback to simple synthesis
        parties = matter.get("parties", [])
        # Handle parties as either list of strings or list of dicts
        party_list = [
            p if isinstance(p, str) else p.get("name", str(p))
            for p in parties
        ] if parties else []
        party_context = ", ".join(party_list)
        lead_issue = issues[0]["issue"]
        cited_strings = [citation.get("cite") for citation in citations if citation.get("cite")]
        if cited_strings:
            cite_text = "; ".join(cited_strings)
            return f"Primary issue '{lead_issue}' for parties {party_context or 'N/A'} is supported by {cite_text}."
        return (
            f"Primary issue '{lead_issue}' for parties {party_context or 'N/A'} lacks direct authority "
            "and requires additional research."
        )
